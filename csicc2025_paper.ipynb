{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMOLsQFuXAVWgXAxF75nOfV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaryamKazemit/OEBGNN/blob/main/csicc2025_paper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General**"
      ],
      "metadata": {
        "id": "ka_Uw9L5kxXU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LLpBm6ur9oj8",
        "outputId": "366221bd-1e33-4fa3-ae8d-b4ad460534bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.6)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torch-geometric scikit-learn imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHMvBeFL90QQ",
        "outputId": "04a2304a-d7ed-4b5a-9583-2f73e52fd3a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from torch_geometric.nn import GCNConv, GATConv, SAGEConv\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "M6jSshsk921u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# enable detailed CUDA debugging\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "metadata": {
        "id": "6B0v-Z0D94WE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "S16zpCk0HEV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Thesis structure***"
      ],
      "metadata": {
        "id": "iHwsqkrEevb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_graph_structure(X, y, similarity_threshold=2.0):\n",
        "    distances = euclidean_distances(X)\n",
        "    edge_index = torch.tensor(\n",
        "        [[i, j] for i in range(len(X)) for j in range(len(X)) if i != j and distances[i, j] < similarity_threshold],\n",
        "        dtype=torch.long\n",
        "    ).t()\n",
        "    x_tensor = torch.tensor(X, dtype=torch.float)\n",
        "    y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "    return Data(x=x_tensor, edge_index=edge_index, y=y_tensor)"
      ],
      "metadata": {
        "id": "LVNJuyI4-XbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_graphsmote(data, minority_class=1, synthetic_ratio=1.5):\n",
        "    minority_mask = data.y == minority_class\n",
        "    minority_indices = torch.where(minority_mask)[0]\n",
        "    minority_features = data.x[minority_indices].numpy()\n",
        "    nn = NearestNeighbors(n_neighbors=5).fit(minority_features)\n",
        "    neighbors = nn.kneighbors(minority_features, return_distance=False)\n",
        "    synthetic_features = []\n",
        "    for i in range(len(minority_features)):\n",
        "        for _ in range(int(synthetic_ratio)):\n",
        "            neighbors_i = neighbors[i]\n",
        "            sampled_neighbor = minority_features[np.random.choice(neighbors_i)]\n",
        "            synthetic_features.append((minority_features[i] + sampled_neighbor) / 2)\n",
        "    synthetic_features = torch.tensor(synthetic_features, dtype=torch.float)\n",
        "    synthetic_labels = torch.tensor([minority_class] * len(synthetic_features), dtype=torch.long)\n",
        "    data.x = torch.cat([data.x, synthetic_features], dim=0)\n",
        "    data.y = torch.cat([data.y, synthetic_labels], dim=0)\n",
        "    num_original_nodes = data.x.size(0) - synthetic_features.size(0)\n",
        "    synthetic_edges = []\n",
        "    for i in range(synthetic_features.size(0)):\n",
        "        synthetic_index = num_original_nodes + i\n",
        "        original_node = np.random.choice(minority_indices.numpy())\n",
        "        synthetic_edges.extend([[synthetic_index, original_node], [original_node, synthetic_index]])\n",
        "    synthetic_edges = torch.tensor(synthetic_edges, dtype=torch.long).t()\n",
        "    data.edge_index = torch.cat([data.edge_index, synthetic_edges], dim=1)\n",
        "    return data"
      ],
      "metadata": {
        "id": "kOHPoqxV-hnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_graph_with_diversity(data, chunk_size):\n",
        "    chunks = []\n",
        "    current_chunk_x, current_chunk_y, current_chunk_edges = [], [], []\n",
        "    for i in range(0, data.num_nodes, chunk_size):\n",
        "        chunk_x = data.x[i:i + chunk_size]\n",
        "        chunk_y = data.y[i:i + chunk_size]\n",
        "        edge_mask = (data.edge_index[0] >= i) & (data.edge_index[0] < i + chunk_size) & \\\n",
        "                    (data.edge_index[1] >= i) & (data.edge_index[1] < i + chunk_size)\n",
        "        chunk_edges = data.edge_index[:, edge_mask] - i\n",
        "        unique_classes = torch.unique(chunk_y)\n",
        "        if len(unique_classes) < 2:\n",
        "            current_chunk_x.append(chunk_x)\n",
        "            current_chunk_y.append(chunk_y)\n",
        "            current_chunk_edges.append(chunk_edges)\n",
        "        else:\n",
        "            if current_chunk_x:\n",
        "                merged_chunk_x = torch.cat(current_chunk_x + [chunk_x], dim=0)\n",
        "                merged_chunk_y = torch.cat(current_chunk_y + [chunk_y], dim=0)\n",
        "                merged_chunk_edges = torch.cat(current_chunk_edges + [chunk_edges], dim=1)\n",
        "                chunks.append(Data(x=merged_chunk_x, edge_index=merged_chunk_edges, y=merged_chunk_y))\n",
        "                current_chunk_x, current_chunk_y, current_chunk_edges = [], [], []\n",
        "            else:\n",
        "                chunks.append(Data(x=chunk_x, edge_index=chunk_edges, y=chunk_y))\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "OMO12rSk-pH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNModel(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.dropout(x)\n",
        "        return self.conv2(x, edge_index)"
      ],
      "metadata": {
        "id": "ilBSWMnx-3qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SAGEModel(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(SAGEModel, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.dropout(x)\n",
        "        return self.conv2(x, edge_index)"
      ],
      "metadata": {
        "id": "83FGkovL-6Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GATModel(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GATModel, self).__init__()\n",
        "        self.conv1 = GATConv(in_channels, hidden_channels, heads=4, concat=True, dropout=0.6)\n",
        "        self.conv2 = GATConv(hidden_channels * 4, out_channels, heads=1, concat=False, dropout=0.6)\n",
        "        self.dropout = torch.nn.Dropout(0.6)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.dropout(x)\n",
        "        return self.conv2(x, edge_index)"
      ],
      "metadata": {
        "id": "OMdD3G4j-8lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnsembleGNN(nn.Module):\n",
        "    def __init__(self, models):\n",
        "        super(EnsembleGNN, self).__init__()\n",
        "        self.models = nn.ModuleList(models)\n",
        "\n",
        "    def forward(self, data):\n",
        "        outputs = [model(data) for model in self.models]\n",
        "        return torch.mean(torch.stack(outputs), dim=0)"
      ],
      "metadata": {
        "id": "5n7ZEMgQ_BkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightedEnsembleGNN(nn.Module):\n",
        "    def __init__(self, models, weights):\n",
        "        super(WeightedEnsembleGNN, self).__init__()\n",
        "        self.models = nn.ModuleList(models)\n",
        "        self.weights = torch.tensor(weights, dtype=torch.float)\n",
        "\n",
        "    def forward(self, data):\n",
        "        outputs = [model(data) for model in self.models]\n",
        "        weighted_outputs = [w * o for w, o in zip(self.weights, outputs)]\n",
        "        return torch.mean(torch.stack(weighted_outputs), dim=0)"
      ],
      "metadata": {
        "id": "A6sfJoFVE29f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, criterion, data, epochs=30, device=\"cpu\"):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "      for chunk in chunks:\n",
        "            chunk = chunk.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(chunk)\n",
        "            loss = criterion(outputs, chunk.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "      print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "Sk56DH1x_E7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, chunks, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    g_means, roc_aucs = [], []\n",
        "    for chunk in chunks:\n",
        "        chunk = chunk.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(chunk)\n",
        "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
        "            labels = chunk.y.cpu().numpy()\n",
        "            if len(np.unique(labels)) < 2:\n",
        "                print(\"Skipping chunk with insufficient class diversity.\")\n",
        "                continue\n",
        "            tn, fp, fn, tp = confusion_matrix(labels, preds, labels=[0, 1]).ravel()\n",
        "            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "            g_mean = np.sqrt(sensitivity * specificity)\n",
        "            roc_auc = roc_auc_score(labels, outputs[:, 1].exp().cpu().numpy())\n",
        "            g_means.append(g_mean)\n",
        "            roc_aucs.append(roc_auc)\n",
        "        print(f\"G-Mean: {g_mean:.4f}, ROC-AUC: {roc_auc:.4f}\")\n",
        "    avg_g_mean = np.nanmean(g_means)\n",
        "    avg_roc_auc = np.nanmean(roc_aucs)\n",
        "    print(f\"Avg G-Mean: {avg_g_mean:.4f}, Avg ROC-AUC: {avg_roc_auc:.4f}\")\n",
        "    return avg_g_mean, avg_roc_auc"
      ],
      "metadata": {
        "id": "3Fndr2jv_JbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_ensemble(models, chunks, device=\"cpu\"):\n",
        "    g_means, roc_aucs = [], []\n",
        "    for chunk in chunks:\n",
        "        chunk = chunk.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = torch.mean(torch.stack([model(chunk) for model in models]), dim=0)\n",
        "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
        "            labels = chunk.y.cpu().numpy()\n",
        "            if len(np.unique(labels)) < 2:\n",
        "                print(\"Skipping chunk with insufficient class diversity.\")\n",
        "                continue\n",
        "            tn, fp, fn, tp = confusion_matrix(labels, preds, labels=[0, 1]).ravel()\n",
        "            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "            g_mean = np.sqrt(sensitivity * specificity)\n",
        "            g_means.append(g_mean)\n",
        "            try:\n",
        "                roc_auc = roc_auc_score(labels, outputs[:, 1].exp().cpu().numpy())\n",
        "                roc_aucs.append(roc_auc)\n",
        "            except ValueError:\n",
        "                print(\"Skipping ROC-AUC calculation for this chunk.\")\n",
        "                continue\n",
        "        print(f\"G-Mean: {g_mean:.4f}, ROC-AUC: {roc_auc:.4f}\")\n",
        "    avg_g_mean = np.nanmean(g_means)\n",
        "    avg_roc_auc = np.nanmean(roc_aucs)\n",
        "    print(f\"Ensemble Avg G-Mean: {avg_g_mean:.4f}, Avg ROC-AUC: {avg_roc_auc:.4f}\")\n",
        "    return avg_g_mean, avg_roc_auc"
      ],
      "metadata": {
        "id": "MEHu3dflAiVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##[Pima Indians Diabetes Database dataset evaluations](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)"
      ],
      "metadata": {
        "id": "lspkzKkN97a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/diabetes.csv'\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "esCgPlXO-Rx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    X = df.drop(columns=['Outcome']).values\n",
        "    y = df['Outcome'].values\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "gYKo_OGl-Vak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main exe of program\n",
        "file_path = '/content/drive/My Drive/diabetes.csv'\n",
        "X, y = preprocess_dataset(file_path)\n",
        "graph_data = create_graph_structure(X, y)\n",
        "balanced_graph_data = apply_graphsmote(graph_data)\n",
        "\n",
        "chunks = chunk_graph_with_diversity(balanced_graph_data,100)\n",
        "\n",
        "gcn_model = GCNModel(in_channels=X.shape[1], hidden_channels=64, out_channels=2).to(device)\n",
        "sage_model = SAGEModel(in_channels=X.shape[1], hidden_channels=64, out_channels=2).to(device)\n",
        "gat_model = GATModel(in_channels=X.shape[1], hidden_channels=64, out_channels=2).to(device)\n",
        "model_weights = [0.5, 0.2, 0.5]\n",
        "ensemble_model = WeightedEnsembleGNN(models=[gcn_model, sage_model, gat_model], weights=model_weights).to(device)\n",
        "# ensemble_model = EnsembleGNN(models=[gcn_model, sage_model, gat_model]).to(device)\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
        "criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float).to(device))\n",
        "optimizer = torch.optim.Adam(ensemble_model.parameters(), lr=0.005, weight_decay=5e-4)\n",
        "\n",
        "train(ensemble_model, optimizer, criterion, chunks, epochs=30, device=device)\n",
        "\n",
        "# seperately\n",
        "print(\"\\nEvaluating GCN...\")\n",
        "gcn_g_mean, gcn_roc_auc = evaluate_model(gcn_model, chunks, device=device)\n",
        "print(\"\\nEvaluating GraphSAGE...\")\n",
        "sage_g_mean, sage_roc_auc = evaluate_model(sage_model, chunks, device=device)\n",
        "print(\"\\nEvaluating GAT...\")\n",
        "gat_g_mean, gat_roc_auc = evaluate_model(gat_model, chunks, device=device)\n",
        "# ensemble form\n",
        "print(\"\\nEvaluating Ensemble...\")\n",
        "ensemble_g_mean, ensemble_roc_auc = evaluate_ensemble([gcn_model, sage_model, gat_model], chunks, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "O9goGuDw_Ppx",
        "outputId": "c1e07b75-d3b6-4407-918c-6f281c2fc215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Loss: 0.5785\n",
            "Epoch [2/30], Loss: 0.5064\n",
            "Epoch [3/30], Loss: 0.4759\n",
            "Epoch [4/30], Loss: 0.4948\n",
            "Epoch [5/30], Loss: 0.4732\n",
            "Epoch [6/30], Loss: 0.4523\n",
            "Epoch [7/30], Loss: 0.5137\n",
            "Epoch [8/30], Loss: 0.4252\n",
            "Epoch [9/30], Loss: 0.4537\n",
            "Epoch [10/30], Loss: 0.4560\n",
            "Epoch [11/30], Loss: 0.4069\n",
            "Epoch [12/30], Loss: 0.4331\n",
            "Epoch [13/30], Loss: 0.4215\n",
            "Epoch [14/30], Loss: 0.4178\n",
            "Epoch [15/30], Loss: 0.4419\n",
            "Epoch [16/30], Loss: 0.4387\n",
            "Epoch [17/30], Loss: 0.4145\n",
            "Epoch [18/30], Loss: 0.4099\n",
            "Epoch [19/30], Loss: 0.4170\n",
            "Epoch [20/30], Loss: 0.4094\n",
            "Epoch [21/30], Loss: 0.4055\n",
            "Epoch [22/30], Loss: 0.4074\n",
            "Epoch [23/30], Loss: 0.4166\n",
            "Epoch [24/30], Loss: 0.3765\n",
            "Epoch [25/30], Loss: 0.4023\n",
            "Epoch [26/30], Loss: 0.4205\n",
            "Epoch [27/30], Loss: 0.3789\n",
            "Epoch [28/30], Loss: 0.4098\n",
            "Epoch [29/30], Loss: 0.3762\n",
            "Epoch [30/30], Loss: 0.3940\n",
            "\n",
            "Evaluating GCN...\n",
            "G-Mean: 0.6731, ROC-AUC: 0.7917\n",
            "G-Mean: 0.6901, ROC-AUC: 0.8251\n",
            "G-Mean: 0.6558, ROC-AUC: 0.7577\n",
            "G-Mean: 0.7313, ROC-AUC: 0.8062\n",
            "G-Mean: 0.6633, ROC-AUC: 0.7879\n",
            "G-Mean: 0.7653, ROC-AUC: 0.8443\n",
            "G-Mean: 0.7854, ROC-AUC: 0.8763\n",
            "G-Mean: 0.7387, ROC-AUC: 0.8506\n",
            "Avg G-Mean: 0.7129, Avg ROC-AUC: 0.8175\n",
            "\n",
            "Evaluating GraphSAGE...\n",
            "G-Mean: 0.6844, ROC-AUC: 0.7804\n",
            "G-Mean: 0.7428, ROC-AUC: 0.8752\n",
            "G-Mean: 0.7381, ROC-AUC: 0.8197\n",
            "G-Mean: 0.7937, ROC-AUC: 0.8468\n",
            "G-Mean: 0.7111, ROC-AUC: 0.8305\n",
            "G-Mean: 0.7352, ROC-AUC: 0.9215\n",
            "G-Mean: 0.6931, ROC-AUC: 0.8399\n",
            "G-Mean: 0.8110, ROC-AUC: 0.8814\n",
            "Avg G-Mean: 0.7387, Avg ROC-AUC: 0.8494\n",
            "\n",
            "Evaluating GAT...\n",
            "G-Mean: 0.6938, ROC-AUC: 0.7926\n",
            "G-Mean: 0.7502, ROC-AUC: 0.8200\n",
            "G-Mean: 0.6763, ROC-AUC: 0.8008\n",
            "G-Mean: 0.7225, ROC-AUC: 0.8096\n",
            "G-Mean: 0.6831, ROC-AUC: 0.7445\n",
            "G-Mean: 0.7653, ROC-AUC: 0.8306\n",
            "G-Mean: 0.7797, ROC-AUC: 0.8288\n",
            "G-Mean: 0.7320, ROC-AUC: 0.8530\n",
            "Avg G-Mean: 0.7254, Avg ROC-AUC: 0.8100\n",
            "\n",
            "Evaluating Ensemble...\n",
            "G-Mean: 0.6731, ROC-AUC: 0.8018\n",
            "G-Mean: 0.7861, ROC-AUC: 0.8820\n",
            "G-Mean: 0.7406, ROC-AUC: 0.8260\n",
            "G-Mean: 0.7645, ROC-AUC: 0.8553\n",
            "G-Mean: 0.7616, ROC-AUC: 0.8357\n",
            "G-Mean: 0.8552, ROC-AUC: 0.9278\n",
            "G-Mean: 0.7656, ROC-AUC: 0.8815\n",
            "G-Mean: 0.8539, ROC-AUC: 0.8979\n",
            "Ensemble Avg G-Mean: 0.7751, Avg ROC-AUC: 0.8635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##[Haberman's Survival dataset evaluation](https://archive.ics.uci.edu/dataset/43/haberman+s+survival)"
      ],
      "metadata": {
        "id": "9hzjKa7uGRRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = '/content/drive/My Drive/Haberman\\'s Survival.zip'\n",
        "extract_path = '/content/haberman_data'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)"
      ],
      "metadata": {
        "id": "9nE6GfbBGl7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset_haberman(file_path):\n",
        "    df = pd.read_csv(file_path, header=None)\n",
        "    df.columns = ['Age', 'Operation_Year', 'Axillary_Nodes', 'Survival_Status']\n",
        "    # Map class labels to 0 and 1\n",
        "    df['Survival_Status'] = df['Survival_Status'].map({1: 0, 2: 1})  # 0: survived, 1: died\n",
        "    print(df.head())\n",
        "    X = df.drop(columns=['Survival_Status']).values\n",
        "    y = df['Survival_Status'].values\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "vG7u4GqZG5QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main exe of program\n",
        "file_path = '/content/haberman_data/haberman.data'\n",
        "X, y = preprocess_dataset_haberman(file_path)\n",
        "graph_data = create_graph_structure(X, y)\n",
        "balanced_graph_data = apply_graphsmote(graph_data)\n",
        "\n",
        "chunks = chunk_graph_with_diversity(balanced_graph_data,100)\n",
        "\n",
        "gcn_model = GCNModel(in_channels=X.shape[1], hidden_channels=64, out_channels=2).to(device)\n",
        "sage_model = SAGEModel(in_channels=X.shape[1], hidden_channels=64, out_channels=2).to(device)\n",
        "gat_model = GATModel(in_channels=X.shape[1], hidden_channels=64, out_channels=2).to(device)\n",
        "model_weights = [0.5, 0.2, 0.5]\n",
        "ensemble_model = WeightedEnsembleGNN(models=[gcn_model, sage_model, gat_model], weights=model_weights).to(device)\n",
        "# ensemble_model = EnsembleGNN(models=[gcn_model, sage_model, gat_model]).to(device)\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
        "criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float).to(device))\n",
        "optimizer = torch.optim.Adam(ensemble_model.parameters(), lr=0.005, weight_decay=5e-4)\n",
        "\n",
        "train(ensemble_model, optimizer, criterion, chunks, epochs=30, device=device)\n",
        "\n",
        "# seperately\n",
        "print(\"\\nEvaluating GCN...\")\n",
        "gcn_g_mean, gcn_roc_auc = evaluate_model(gcn_model, chunks, device=device)\n",
        "print(\"\\nEvaluating GraphSAGE...\")\n",
        "sage_g_mean, sage_roc_auc = evaluate_model(sage_model, chunks, device=device)\n",
        "print(\"\\nEvaluating GAT...\")\n",
        "gat_g_mean, gat_roc_auc = evaluate_model(gat_model, chunks, device=device)\n",
        "# ensemble form\n",
        "print(\"\\nEvaluating Ensemble...\")\n",
        "ensemble_g_mean, ensemble_roc_auc = evaluate_ensemble([gcn_model, sage_model, gat_model], chunks, device=device)"
      ],
      "metadata": {
        "id": "HOeZEZtlbLUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Stanford Twitter Stream dataset evaluation](https://jmgomezhidalgo.blogspot.com/2013/01/a-list-of-datasets-for-opinion-mining.html)"
      ],
      "metadata": {
        "id": "mdOlYBhusqT4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nJe9xQRwspsC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}